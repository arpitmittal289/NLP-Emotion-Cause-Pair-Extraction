# -*- coding: utf-8 -*-
"""Clause Extraction and Data Annotation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cgUzQQNbACVe-WJmimo22H3B9cZPjhd6
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

# !pip install benepar
# !pip install pycorenlp

# !pip install spacy --upgrade

# !python -m spacy download en_core_web_trf

import pandas as pd
import numpy as np
import torch.nn as nn
from torch.optim import SGD
import random
import warnings
import re
warnings.filterwarnings('ignore')
from torch import save
import torch
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.sampler import SubsetRandomSampler
from torch import load as model_load, FloatTensor, LongTensor, tensor as Tensor
from torch import max as output_max
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
import gc
from nltk.tokenize import TweetTokenizer
import nltk
nltk.download('wordnet', quiet=True)
import json
import benepar   
import nltk
import re
from pycorenlp import *
import spacy
import pickle

nltk.download('punkt')
nlp = spacy.load('en_core_web_trf')

import pandas as pd
import json
dictionaries = []

# Change path if necessary
with open('dataset.json', 'r') as file:
    data = file.readlines()
    for row in data:
        dictionaries.append(json.loads(row))

df = pd.DataFrame(dictionaries)
df.head()

del dictionaries

reviews = []
from collections import defaultdict
for row_ind, row in df.iterrows():
  reviews.append(row['reviewText'])

# print(reviews)
print(len(reviews))

with open('reviews_for_embeddings.pkl', 'wb') as file:
  pickle.dump(reviews, file)

with open('reviews_for_embeddings.pkl', 'rb') as file:
  rv = pickle.load(file)

"""Clause Extraction given a review sentence"""

def find_root_of_sentence(doc):
    root_token = None
    for token in doc:
        if (token.dep_ == "ROOT"):
            root_token = token
    return root_token

def find_other_verbs(doc, root_token):
    other_verbs = []
    for token in doc:
        ancestors = list(token.ancestors)
        if (token.pos_ == "VERB" and len(ancestors) == 1\
            and ancestors[0] == root_token):
            other_verbs.append(token)
    return other_verbs

def get_clause_token_span_for_verb(verb, doc, all_verbs):
    first_token_index = len(doc)
    last_token_index = 0
    this_verb_children = list(verb.children)
    for child in this_verb_children:
        if (child not in all_verbs):
            if (child.i < first_token_index):
                first_token_index = child.i
            if (child.i > last_token_index):
                last_token_index = child.i
    return(first_token_index, last_token_index)


def extract_clauses(document):
    clauses = []
    words = document.split(' ')
    if (len(words) <= 5):
        return [document]
    num_words = len(words)
    idx = 0
    while (num_words > 0):
        clause_size = min(np.random.randint(4, 9), num_words)
        if num_words > clause_size and num_words - clause_size < 3 and len(words) >= 8:
            clause_size = num_words // 2
        clauses.append(' '.join(words[idx:idx + clause_size]))
        idx += clause_size
        num_words -= clause_size
    return clauses

def add_punctuation(review):
    last = review[-1]
    if last not in ['.','?','!']:
        review = ''.join((review, '.'))
    return review

## ALL DATA CLEANING FUNCTIONS
def RemoveHTMLAndURLs(text):
    return re.sub("([^\s]+://)?([^\s]+\.)+[^\s]+", "", re.sub(r'<.*?>', '', text))
def RemoveExtraSpaces(text):
    return re.sub("\s+", " ", text)
def RemoveHashtags(text):
    return re.sub("#[^\s]+", "", text)
def ExpandContractions(text):
    text = re.sub("won't", "will not", text)
    text = re.sub("can't", "cannot", text)
    text = re.sub("shan't", "shall not", text)
    text = re.sub("(?:[a-z])'re", " are", text)
    text = re.sub("(?:[a-z])'d", " would", text)
    text = re.sub("(?:[a-z])'ll", " will", text)
    text = re.sub("(?:[a-z])'ve", " have", text)
    text = re.sub("n't", " not", text)
    text = re.sub("o'clock", "of the clock", text)
    return text
def RemoveSpecialChar(text):
    return re.sub("[^A-Za-z\s]", "", text)
def TrimWords(words, max_word_size):
    return [word[:max_word_size-1] for word in words]

def tokenize_clauses(clauses, tokenize_fn, max_word_size=20):
    tokenized = []
    for clause in clauses:
        tokenized.append(TrimWords(tokenize_fn(RemoveSpecialChar(ExpandContractions(RemoveHashtags((RemoveExtraSpaces(RemoveHTMLAndURLs(clause))))))),
                                                              max_word_size))
    return tokenized

def get_vocab(corpus):
    word_list = ['<PAD>', '<UNK>']
    for clause_it in corpus:
        word_list.extend(np.concatenate(clause_it))
    return np.unique(word_list)

def encode_words(clauses, vocab, max_clause_size=20):
    encoded = []
    for clause in clauses:
        encoded_clause = [ np.argmax(vocab == word) if word in vocab else 1 for word in clause ] 
        # pad to max_clause_size
        encoded_clause.extend([0] * (20 - len(encoded_clause)))
        encoded.append(encoded_clause)
    return encoded

def pad_with_clauses(corpus, num_words, max_clauses):
    for doc in corpus:
        for _ in range(max_clauses - len(doc)):
            doc.append([0] * num_words)

def predict_clauses(review_sent, max_num_clauses):
    review_ds = []
    for r in range(len(review_sent)):
        temp = []
        for s in range(len(review_sent[r])):
            sent = review_sent[r][s]
            doc = nlp(sent)
            root_token = find_root_of_sentence(doc)
            other_verbs = find_other_verbs(doc, root_token)

            token_spans = []   
            all_verbs = [root_token] + other_verbs
            for other_verb in all_verbs:
                (first_token_index, last_token_index) = \
                get_clause_token_span_for_verb(other_verb, doc, all_verbs)
                token_spans.append((first_token_index, last_token_index))

            sentence_clauses = []
            for token_span in token_spans:
                start = token_span[0]
                end = token_span[1]
                if (start < end):
                    clause = doc[start:end]
                    sentence_clauses.append(clause)

            sentence_clauses = sorted(sentence_clauses, key=lambda tup: tup[0])
            clauses_text = [clause.text for clause in sentence_clauses]
            
            if len(clauses_text) > 1:
                for i in range(1, len(clauses_text)):
                    idx = clauses_text[i-1].find(clauses_text[i])
                    if idx > 0:
                        f = clauses_text[i-1][:idx]
                        l = clauses_text[i-1][idx:]
                    # else:
                    #     print(clauses_text[i-1], clauses_text[i], idx)
                        clauses_text[i-1] = f
                        clauses_text[i] = l

            temp += clauses_text
            
        if len(temp) > max_num_clauses:
            temp = temp[:max_num_clauses]


        review_ds.append(temp)

    return review_ds

def get_clause_list(review : str):
  try:
    review = add_punctuation(review)
    sentences = [nltk.sent_tokenize(review)]
    clauses = predict_clauses(sentences, 100)
    return clauses
  except AttributeError as e:
    print(e)
    return []

"""Selecting products and reviews to be annotated"""

from collections import defaultdict
selected_asins = defaultdict(lambda : [])
for row_ind, row in df.iterrows():
  if row_ind % 10000 == 0:
    print(row_ind, df.shape[0])
  asin = row['asin']
  if asin not in selected_asins and len(selected_asins) >= 50:
    break
  if len(row['reviewText']) <= 300 and len(row['reviewText']) > 0 and len(selected_asins[asin]) <= 20:
    selected_asins[asin].append((row['reviewText'], row['summary'], get_clause_list(row['reviewText']).extend(get_clause_list(row['summary']))))

"""Creating batches of data so that every team member can annotate parallely"""

asin_count = 0
index = 0
send_data = [{} for _ in range(5)]
names = ['A', 'B', 'C', 'D', 'E']
for asin in selected_asins:
  send_data[index][asin] = selected_asins[asin]
  asin_count += 1
  if asin_count == 10:
    with open('send_data_' + names[index] + '.pkl', 'wb') as file:
      pickle.dump(send_data[index], file)
    index += 1
    asin_count = 0

asin_count = 0
index = 0
send_data = [{} for _ in range(5)]
for asin in selected_asins:
  send_data[index][asin] = selected_asins[asin]
  print(send_data)
  asin_count += 1
  if asin_count == 1:
    with open('send_data_sample.pkl', 'wb') as file:
      pickle.dump(send_data[index], file)
    index += 1
    asin_count = 0
    break

"""Annotation Assisting Script"""

import pickle
name = 'A'
file_name = 'send_data_' + name + '.pkl'

with open(file_name, 'rb') as file:
  data_dict = pickle.load(file)

df_rows = []
for selected_asin in data_dict:
  data = data_dict[selected_asin]
  for review, summary, clauses in data:
    print('Product ID :', selected_asin)
    print('Review :', review)
    print('Summary :', summary)
    print('Clauses:')
    for ind in range(len(clauses[0])):
      print(ind, clauses[0][ind])

    # Emotion indices : 
    # 0 anger
    # 1 anticipation
    # 2 disgust
    # 3 fear
    # 4 joy
    # 5 sadness
    # 6 surprise
    # 7 trust

    emotion = int(input('Enter emotion'))
    if emotion < 0 or emotion > 7:
      print('Error - Invalid emotion')
      break
    clause_index = int(input('Enter clause index'))
    if clause_index >= len(clauses[0]) or clause_index < 0:
      print('Error - Invalid clause index')
      break
    df_rows.append((selected_asin, review, summary, clauses, emotion, clause_index))
with open('result_' + name + '.pkl', 'wb') as file:
  pickle.dump(df_rows, file)

"""Merge Every Team-member's Annotated Datasets"""

result_names = ['A', 'B', 'C', 'D', 'E']
df_list = [arpit_df]
for result_name in result_names:
  file_name = 'result_' + result_name + '.pkl'
  with open(file_name, 'rb') as file:
    cur_result = pickle.load(file)
    cur_df = pd.DataFrame(cur_result, columns = arpit_df.columns)
    df_list.append(cur_df)
new_df = pd.concat(df_list, axis = 0, ignore_index=True)

with open('final_result.pkl', 'wb') as file:
  pickle.dump(new_df, file)

"""Add embeddings"""

with open('emotionAwareEmbeddings.pkl', 'rb') as file:
  eae = pickle.load(file)

from nltk import word_tokenize
def get_sentence_embedding_eae(sentence : str):
  sentence = sentence.replace(' \'', ' ').replace('..', ' ')
  words = [x.lower() for x in word_tokenize(sentence)]
  embedding = []
  for word in words:
    embedding.append(eae[word])
  return np.array(embedding)
get_sentence_embedding_eae(new_df['Review'][0]).shape

new_df['review_text_eae_embeddings'] = new_df['Review'].apply(lambda x : get_sentence_embedding_eae(str(x)))
new_df.head()

def clean_clauses(clauses):
  if type(clauses[0]) == type([]):
    return clauses[0]
  return clauses

new_df['Clauses'] = new_df['Clauses'].apply(lambda x : clean_clauses(x))

new_df['clause_eae_embeddings'] = new_df['Clauses'].apply(lambda x : np.array([get_sentence_embedding_eae(str(clause)) for clause in x]))
new_df.head()

with open('wordEmbeddings.pkl', 'rb') as file:
  we = pickle.load(file)

from nltk import word_tokenize
def get_sentence_embedding(sentence : str):
  sentence = sentence.replace(' \'', ' ').replace('..', ' ')
  words = [x.lower() for x in word_tokenize(sentence)]
  embedding = []
  for word in words:
    embedding.append(we[word])
  return np.array(embedding)
get_sentence_embedding(new_df['Review'][0]).shape

new_df['review_text_we_embeddings'] = new_df['Review'].apply(lambda x : get_sentence_embedding(str(x)))
new_df.head()

new_df['clause_we_embeddings'] = new_df['Clauses'].apply(lambda x : np.array([get_sentence_embedding(str(clause)) for clause in x]))
new_df.head()

with open('annotated_dataset.pkl', 'wb') as file:
  pickle.dump(new_df, file)