# -*- coding: utf-8 -*-
"""Cluster Review Clauses.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HNL6GbM9F8bgHw_WWUP3Coox7heCSXCZ
"""

import pandas as pd
import pickle

import nltk
nltk.download('punkt')

with open('/content/predicted_dataset.pkl', 'rb') as file:
  df = pickle.load(file)
df.head()

from nltk import word_tokenize
def get_clause_tokens(clause : str):
  clause = clause.replace(' \'', ' ').replace('..', ' ')
  words = [x.lower() for x in word_tokenize(clause)]
  return words

"""**Getting Clause Embeddings** : Combined emotion-aware embeddings and word2vec embeddings for all words and then max-pooled across the words of the clause"""

from collections import defaultdict
data = defaultdict(lambda : ([], []))
for ind, row in df.iterrows():
  causeClauseIndex = int(row['Predicted Cause Index'])
  clause = row['clauses'][causeClauseIndex]
  emotion = row['emotion']
  clause_tokens = get_clause_tokens(clause)
  clause_eae_embeddings = row['clause_eae_embeddings'][causeClauseIndex]
  clause_we_embeddings = row['clause_we_embeddings'][causeClauseIndex]
  
  we_embedding_length = len(clause_we_embeddings[0])
  eae_embedding_length = len(clause_eae_embeddings[0])

  clause_embeddings = [0] * (we_embedding_length + eae_embedding_length)

  for wind in range(len(clause_tokens)):
    for eind in range(we_embedding_length):
      clause_embeddings[eind] = max(clause_embeddings[eind], clause_we_embeddings[wind][eind])
    for eind in range(eae_embedding_length):
      clause_embeddings[eind + we_embedding_length] = max(clause_embeddings[eind + we_embedding_length], clause_eae_embeddings[wind][eind])
  data[(row['ProductID'], emotion)][0].append(clause)
  data[(row['ProductID'], emotion)][1].append(clause_embeddings)

"""Clustering the reviews for each (product, emotion) pair. Here, agglomerative clustering is used. For distance between two clusters we compute cosine similarity between every pair of elements in the two cluster and the max distance is considered (complete linkage with cosine similarity). We keep joining clusters till distance is less than 0.13.

Selecting head clause for cluster : Select the clause which is close to every clause in cluster. (minimizing the max distance of a clause from the head clause in the cluster)
"""

from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity

cluster_labels = {}

for product, emotion in data:
  if ((product, emotion)) == ('3744295508', 4):
    clauses = data[(product, emotion)][0]
    vectors = data[(product, emotion)][1]

    print('Product ID:', product)
    print('Emotion: joy')
    print('\n')

    ind = 1
    print('Cause Clause List: ')
    for clause in clauses:
      print(str(ind) + '. ' + clause)
      ind += 1
    print('\n')

    if len(vectors) <= 1:
      continue

    clustering = AgglomerativeClustering(distance_threshold = 0.13, linkage = 'complete', n_clusters=None, affinity = 'cosine').fit(vectors)
    cluster_indices = clustering.labels_
    cluster_labels[(product, emotion)] = cluster_indices
    clusters = [[] for _ in range(max(cluster_indices) + 1)]

    for ind in range(len(cluster_indices)):
      clusters[cluster_indices[ind]].append((clauses[ind], vectors[ind]))
    
    results = []
    cluster_ind = 1
    for cluster in clusters:
      if len(cluster) > 1:
        average_embedding = [0] * len(cluster[0][1])
        clauses = []

        for clause, embedding in cluster:
          for eind in range(len(embedding)):
            average_embedding[eind] += embedding[eind] / len(cluster)
          clauses.append(clause)
        print('Cluster', cluster_ind)
        cind = 1
        for clause in clauses:
          print('Clause-' + str(cind) + ': ' + clause)
          cind += 1

        selected_clause = ''
        best_cos_sim = -1
        for clause, embedding in cluster:
          cur_min_cos_sim = 1.1
          for sclause, sembedding in cluster:
            cos_sim = cosine_similarity([sembedding], [embedding])[0]
            cur_min_cos_sim = min(cur_min_cos_sim, cos_sim)
          if cur_min_cos_sim >= best_cos_sim:
            best_cos_sim = cur_min_cos_sim
            selected_clause = clause
        results.append(selected_clause)
        print('Head Clause:', selected_clause)
        print('\n')
        cluster_ind += 1
    print('Final Clauses:')
    cind = 1
    for clause in results:
      print('Clause-' + str(cind) + ': ' + clause)
      cind += 1

from sklearn.manifold import TSNE
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
import numpy as np
from random import randint

X = TSNE(n_components=2, metric = 'euclidean').fit_transform(np.array(data[('3744295508', 4)][1]))

colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']
color = []
for i in range(20):
  color.append('#%06X' % randint(0, 0xFFFFFF))

plt.scatter([x[0] for x in X], [x[1] for x in X], c = cluster_labels[('3744295508', 4)], s=200, cmap=ListedColormap(color))
plt.legend()
plt.show()

from collections import defaultdict

selected_clauses = {}

for product, emotion in data:
  clauses = data[(product, emotion)][0]
  vectors = data[(product, emotion)][1]

  if len(vectors) <= 1:
    continue

  clustering = AgglomerativeClustering(distance_threshold = 0.13, linkage = 'complete', n_clusters=None, affinity = 'cosine').fit(vectors)
  cluster_indices = clustering.labels_
  cluster_labels[(product, emotion)] = cluster_indices
  clusters = [[] for _ in range(max(cluster_indices) + 1)]

  for ind in range(len(cluster_indices)):
    clusters[cluster_indices[ind]].append((clauses[ind], vectors[ind]))
  
  results = []
  cluster_ind = 1
  for cluster in clusters:
    if len(cluster) > 1:
      average_embedding = [0] * len(cluster[0][1])
      clauses = []

      for clause, embedding in cluster:
        for eind in range(len(embedding)):
          average_embedding[eind] += embedding[eind] / len(cluster)
        clauses.append(clause)

      selected_clause = ''
      best_cos_sim = -1
      for clause, embedding in cluster:
        cur_min_cos_sim = 1.1
        for sclause, sembedding in cluster:
          cos_sim = cosine_similarity([sembedding], [embedding])[0]
          cur_min_cos_sim = min(cur_min_cos_sim, cos_sim)
        if cur_min_cos_sim >= best_cos_sim:
          best_cos_sim = cur_min_cos_sim
          selected_clause = clause
      results.append(selected_clause)
      cluster_ind += 1
  selected_clauses[(product, emotion)] = results

with open('final_results.pkl', 'wb') as file:
  pickle.dump(selected_clauses, file)